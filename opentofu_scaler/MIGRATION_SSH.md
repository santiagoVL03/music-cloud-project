# Migraci√≥n de S3 a SSH para Join Token

## üîç Problema Identificado

El script de inicializaci√≥n del master fallaba al intentar subir el `join-command.sh` a S3 debido a:

```
An error occurred (AccessDenied) when calling the ListBuckets operation: 
User: arn:aws:sts::800114385106:assumed-role/k8s-node-role/i-011d61a2e089bfb79 
is not authorized to perform: s3:ListAllMyBuckets
```

**Causas:**
1. El IAM role no ten√≠a permisos de `s3:ListAllMyBuckets`
2. Dependencia innecesaria de servicios externos (S3, IAM, AWS CLI)
3. Mayor complejidad y puntos de fallo

## ‚úÖ Soluci√≥n Implementada

### Cambio de Arquitectura: S3 ‚Üí SSH

En lugar de subir el token a S3 y descargarlo, ahora los workers **obtienen el join command directamente del master via SSH**.

### Ventajas
- ‚úÖ **M√°s simple**: No depende de S3, IAM, ni AWS CLI
- ‚úÖ **M√°s r√°pido**: Conexi√≥n directa entre nodos
- ‚úÖ **M√°s seguro**: Usa la misma SSH key que ya est√° configurada
- ‚úÖ **Menos puntos de fallo**: Solo requiere conectividad de red local
- ‚úÖ **M√°s econ√≥mico**: No usa servicios AWS adicionales

---

## üìù Cambios Realizados

### 1. **master-init.sh**

**ANTES:**
```bash
# Subir el comando de join a S3
aws s3 cp /tmp/join-command.sh s3://${s3_bucket}/join-command.sh --region ${aws_region}
```

**AHORA:**
```bash
# Guardar el comando join localmente para acceso via SSH
sudo kubeadm token create --print-join-command > /tmp/join-command.sh
sudo chmod 644 /tmp/join-command.sh
log "‚úì Comando de join guardado en /tmp/join-command.sh"
log "Los workers se conectar√°n via SSH para obtenerlo"
```

**Eliminado:**
- Instalaci√≥n de AWS CLI (ahorra ~2 minutos de setup)
- Subida a S3
- Referencias a `${s3_bucket}` y `${aws_region}`

---

### 2. **worker-init.sh**

**ANTES:**
```bash
# Descargar desde S3
aws s3 cp s3://${s3_bucket}/join-command.sh /tmp/join-command.sh --region ${aws_region}
```

**AHORA:**
```bash
# Obtener via SCP (SSH)
sudo -u ubuntu scp -o ConnectTimeout=5 \
  ubuntu@${master_ip}:/tmp/join-command.sh \
  /tmp/join-command.sh
```

**Configuraci√≥n SSH autom√°tica:**
```bash
mkdir -p /home/ubuntu/.ssh
cat <<EOF > /home/ubuntu/.ssh/config
Host master
    HostName ${master_ip}
    User ubuntu
    StrictHostKeyChecking no
    UserKnownHostsFile /dev/null
EOF
chmod 600 /home/ubuntu/.ssh/config
```

**Eliminado:**
- Instalaci√≥n de AWS CLI
- Referencias a S3 bucket
- Dependencia de IAM roles

---

### 3. **main.tf**

**Simplificaci√≥n de templates:**

```hcl
# Master - ANTES
user_data = templatefile("${path.module}/scripts/master-init.sh", {
  pod_network_cidr = "10.244.0.0/16"
  s3_bucket        = aws_s3_bucket.k8s_token_bucket.id
  aws_region       = var.aws_region
})

# Master - AHORA
user_data = templatefile("${path.module}/scripts/master-init.sh", {
  pod_network_cidr = "10.244.0.0/16"
})

# Worker - ANTES
user_data = templatefile("${path.module}/scripts/worker-init.sh", {
  master_ip  = aws_instance.k8s_master.private_ip
  s3_bucket  = aws_s3_bucket.k8s_token_bucket.id
  aws_region = var.aws_region
})

# Worker - AHORA
user_data = templatefile("${path.module}/scripts/worker-init.sh", {
  master_ip = aws_instance.k8s_master.private_ip
})
```

**Nota:** El bucket S3 e IAM role a√∫n existen en el c√≥digo (se pueden eliminar en el futuro), pero ya no se usan.

---

## üöÄ Pr√≥ximos Pasos

### Para aplicar los cambios en tu cluster actual:

**Opci√≥n 1: Recrear Workers (Recomendado)**
```bash
cd opentofu_scaler/

# Destruir solo los workers
tofu destroy -target=aws_instance.k8s_worker

# Recrearlos con el nuevo script
tofu apply -target=aws_instance.k8s_worker
```

**Opci√≥n 2: Join Manual (M√°s r√°pido)**
```bash
# En el master
ssh ubuntu@98.92.139.147
sudo kubeadm token create --print-join-command

# Copiar el output y ejecutarlo en cada worker
ssh ubuntu@<worker-ip>
sudo <paste-join-command>
```

---

## üîê Seguridad

**¬øEs seguro desactivar StrictHostKeyChecking?**

Para ambientes de producci√≥n, deber√≠as:
1. Usar `StrictHostKeyChecking yes`
2. Pre-poblar `known_hosts` con la fingerprint del master
3. O usar un sistema de gesti√≥n de configuraci√≥n (Ansible, Chef, etc.)

Para desarrollo/testing, `StrictHostKeyChecking no` es aceptable ya que:
- Los nodos est√°n en una VPC privada
- Usas SSH keys (no passwords)
- Es solo para la fase de bootstrap (despu√©s se borra la config)

---

## üìä Comparaci√≥n de Tiempos

| Componente | S3 (Antes) | SSH (Ahora) |
|------------|------------|-------------|
| Instalaci√≥n AWS CLI | ~120s | 0s ‚úÖ |
| Subida/Descarga | ~5-10s | ~1s ‚úÖ |
| Reintentos en fallo | 30 x 10s = 5min | 30 x 10s = 5min |
| **Total (√©xito)** | **~130s** | **~1s** |
| **Total (fallo)** | **~430s** | **~301s** |

---

## ‚úÖ Verificaci√≥n

Despu√©s de recrear los workers, verifica:

```bash
# En tu m√°quina local
ssh ubuntu@98.92.139.147

# En el master
kubectl get nodes
# Deber√≠as ver: k8s-master, k8s-worker-1, k8s-worker-2 (todos Ready)

kubectl get pods -A
# Todos los pods deber√≠an estar Running

# Logs de los workers
ssh ubuntu@<worker-ip>
sudo cat /var/log/k8s-setup/worker-complete.log | grep "SSH"
# Deber√≠as ver: "‚úì Comando de join obtenido exitosamente via SSH!"
```

---

## üêõ Troubleshooting

**Si los workers no se pueden conectar al master via SSH:**

1. Verifica conectividad:
   ```bash
   ssh ubuntu@<worker-ip>
   ping <master-private-ip>
   nc -zv <master-private-ip> 22
   ```

2. Verifica que el archivo existe en el master:
   ```bash
   ssh ubuntu@<master-ip>
   ls -la /tmp/join-command.sh
   cat /tmp/join-command.sh
   ```

3. Verifica los logs del worker:
   ```bash
   ssh ubuntu@<worker-ip>
   sudo tail -f /var/log/k8s-setup/worker-complete.log
   ```

4. Prueba SCP manual:
   ```bash
   ssh ubuntu@<worker-ip>
   scp ubuntu@<master-private-ip>:/tmp/join-command.sh /tmp/test.sh
   ```

---

## üìö Archivos Modificados

- ‚úèÔ∏è `scripts/master-init.sh` - Eliminada subida a S3, eliminado AWS CLI
- ‚úèÔ∏è `scripts/worker-init.sh` - Cambiado S3 por SCP, eliminado AWS CLI
- ‚úèÔ∏è `main.tf` - Simplificados templates (menos variables)
- üìÑ `MIGRATION_SSH.md` - Este documento

---

**Fecha de migraci√≥n:** 2025-11-04
**Motivaci√≥n:** Fallo de permisos S3, simplificaci√≥n de arquitectura
**Impacto:** Reducci√≥n de dependencias, mayor velocidad, mayor confiabilidad
